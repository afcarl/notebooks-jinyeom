{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "Jin Yeom (jinyeom@utexas.edu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll study and experiment with basics of reinforcement learning (RL). We're going to first talk about RL techniques that are based on known MDPs, in which every state, every transition from one to another, and its reward is exposed to the agent; then we'll talk about methods with unknown MDPs, where the agent has to explore the environment, from one state to another, to approximate the MDP.\n",
    "\n",
    "Note that this notebook is loosely based on Sutton and Barto's book __Reinforcement Learning: An introduction__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration and Policy Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we're going to start with RL techniques based on known MDPs. The model is provided to the agent, so the agent has a direct access to each state. It will explore and evaluate each state, so that the agent can learn to decide whether it should be in a certain state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Bellman Equation\n",
    "\n",
    "One key idea for the two algorithms in this section is the Bellman Equation (shown below).\n",
    "\n",
    "$$\n",
    "V^*(s) = \\max_{a}\\sum_{s'} T(s, a, s')[R(s, a, s') + \\gamma V^*(s')]\n",
    "$$\n",
    "\n",
    "One important insight from this equation is that the optimal value of each state is determined by that of the next state. Dynamic programming can be used to efficiently solve the recurrence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridWorld\n",
    "\n",
    "For this section, we'll experiment a small problem called GridWorld. In this problem, the agent moves from a grid to another to reach the goal, while avoiding fire. The code for GridWorld is included separately in this repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from scripts import gridworld as gw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with a common example grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-+-+-+-+\n",
      "| | | |o|\n",
      "+-+-+-+-+\n",
      "| |#| |x|\n",
      "+-+-+-+-+\n",
      "| | | | |\n",
      "+-+-+-+-+\n"
     ]
    }
   ],
   "source": [
    "# Aliases for easier interpretation of each grid type.\n",
    "D, W, G, F = gw.GRID, gw.WALL, gw.GOAL, gw.FIRE\n",
    "\n",
    "grids = gw.GridWorld(np.array([[D, D, D, G],\n",
    "                               [D, W, D, F],\n",
    "                               [D, D, D, D]]))\n",
    "\n",
    "# Visualize the grids.\n",
    "# Empty grids are ones that are reachable, `#` shows a wall,\n",
    "# `o` is the good terminal state (goal), and `x` is the bad\n",
    "# terminal state (fire).\n",
    "grids.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Iteration\n",
    "Value Iteration algorithm simply computes the value of each state iteratively, using the recurrence of the Bellman Equation; hence the name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, gamma):\n",
    "    V = np.zeros(env.shape)\n",
    "    while True:\n",
    "        V_ = V\n",
    "        for s in env.S:\n",
    "            V_[s] = sum([p * (env.R(s, a) + gamma * V[env.T(s, a)]) \n",
    "                         for a, p in enumerate(policy[s])])\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Iteration\n",
    "Policy Iteration algorithm follows the steps below:\n",
    "1. Initialize the agent's policy ($\\forall s \\in S, \\pi(s) \\in A$)\n",
    "2. Evaluate $\\pi$ for each state $s$.\n",
    "3. Improve $\\pi$ for each state $s$; go to 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-+-+-+-+\n",
      "|^|^|^|^|\n",
      "+-+-+-+-+\n",
      "|^|^|^|^|\n",
      "+-+-+-+-+\n",
      "|^|^|^|^|\n",
      "+-+-+-+-+\n"
     ]
    }
   ],
   "source": [
    "def init_policy(env):\n",
    "    \"\"\" Initialize the policy with 4 possible actions for each state with the\n",
    "    same probability (1.0 / 4 actions = 0.25). \"\"\"\n",
    "    r, c = env.shape\n",
    "    return np.full((r, c, 4), 0.25)\n",
    "\n",
    "pi = init_policy(grids)\n",
    "grids.show(policy=pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.20431734  0.05373022  0.20431734 -0.03712508]\n",
      " [-0.284497   -0.06847337 -0.284497   -0.09822688]\n",
      " [-0.09756946 -0.03252315 -0.09756946 -0.36596606]]\n"
     ]
    }
   ],
   "source": [
    "def eval_policy(policy, env, gamma, theta=1e-8):\n",
    "    \"\"\" Evaluate the argument policy. \"\"\"\n",
    "    V = np.zeros(env.shape)\n",
    "    while True:\n",
    "        delta = 0.0\n",
    "        for s in env.S:\n",
    "            v = V[s]\n",
    "            V[s] = sum([p * (env.R(s, a) + gamma * V[env.T(s, a)])\n",
    "                        for a, p in enumerate(policy[s])])\n",
    "            delta = max(delta, abs(v - V[s]))\n",
    "        if delta < theta:\n",
    "            return V\n",
    "        \n",
    "print(eval_policy(pi, grids, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impr_policy(policy, env, gamma):\n",
    "    \"\"\" Improve the argument policy. \"\"\"\n",
    "    policy_stable = True\n",
    "    V = eval_policy(policy, env, gamma)\n",
    "    for s in env.S:\n",
    "        old_action = np.argmax(policy[s])\n",
    "        new_action = np.argmax([p * (env.R(s) + gamma * V[env.T(s, a)])\n",
    "                               for a, p in enumerate(policy[s])])\n",
    "        if new_action != old_action:\n",
    "            policy_stable = False\n",
    "        policy[s] = np.eye(4)[new_action]\n",
    "    return policy_stable, V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can put these functions together to complete Policy Iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-+-+-+-+\n",
      "|<|<|>|>|\n",
      "+-+-+-+-+\n",
      "|^|^|^|^|\n",
      "+-+-+-+-+\n",
      "|^|^|^|^|\n",
      "+-+-+-+-+\n",
      "[[ 8.99999991  8.09999992  8.99999991  9.99999991]\n",
      " [ 8.09999992  7.28999993  8.09999992  7.99999992]\n",
      " [ 7.28999993  0.          7.28999993  7.19999993]]\n"
     ]
    }
   ],
   "source": [
    "def policy_iteration(env, gamma):\n",
    "    pi = init_policy(env)\n",
    "    done = False\n",
    "    while not done:\n",
    "        done, V = impr_policy(pi, env, gamma)\n",
    "    return pi, V\n",
    "\n",
    "pi, V = policy_iteration(grids, 0.9)\n",
    "grids.show(policy=pi)\n",
    "print(V)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
