{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "Author: Jin Yeom (jinyeom@utexas.edu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll study and experiment with basics of reinforcement learning (RL). We're going to first talk about RL techniques that are based on known MDPs, in which every state, every transition from one to another, and its reward is exposed to the agent; then we'll talk about methods with unknown MDPs, where the agent has to explore the environment, from one state to another, to approximate the MDP.\n",
    "\n",
    "Note that this notebook is loosely based on Sutton and Barto's book __Reinforcement Learning: An introduction__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration and Policy Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we're going to start with RL techniques based on known MDPs. The model is provided to the agent, so the agent has a direct access to each state. It will explore and evaluate each state, so that the agent can learn to decide whether it should be in a certain state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Bellman Equation\n",
    "\n",
    "One key idea for the two algorithms in this section is the Bellman Equation (shown below).\n",
    "\n",
    "$$\n",
    "V^*(s) = \\max_{a}\\sum_{s'} T(s, a, s')[R(s, a, s') + \\gamma V^*(s')]\n",
    "$$\n",
    "\n",
    "One important observation from this equation is that the optimal value of each state is determined by that of the next state. Dynamic programming can be used to efficiently solve the recurrence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For expermenting with these algorithms, we're going to use a GridWorld package developed by DeepMind, called pycolab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-0b4847faeaf5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-0b4847faeaf5>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   \u001b[0;31m# Let the game begin!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m   \u001b[0mui\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pycolab-1.0-py3.6.egg/pycolab/human_ui.py\u001b[0m in \u001b[0;36mplay\u001b[0;34m(self, game)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;31m# After turning on curses, set it up and play the game.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m     \u001b[0mcurses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_curses_and_play\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;31m# The game has concluded. Print the final statistics.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/curses/__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(func, *args, **kwds)\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstdscr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;31m# Set everything back to normal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pycolab-1.0-py3.6.egg/pycolab/human_ui.py\u001b[0m in \u001b[0;36m_init_curses_and_play\u001b[0;34m(self, screen)\u001b[0m\n\u001b[1;32m    258\u001b[0m       \u001b[0;31m# play() method.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m       \u001b[0melapsed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_display\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_total_return\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melapsed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m       \u001b[0;31m# Update game console message buffer with new messages from the game.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pycolab-1.0-py3.6.egg/pycolab/human_ui.py\u001b[0m in \u001b[0;36m_display\u001b[0;34m(self, screen, observation, score, elapsed)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;31m# Redraw the game screen (but in the curses memory buffer only).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m     \u001b[0mscreen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnoutrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_update_game_console\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_log_messages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsole\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaint_console\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Iteration\n",
    "Value Iteration algorithm simply computes the value of each state iteratively until it converges, using the recurrence in the Bellman Equation; hence the name. So the equation above can be rewritten in the following form,\n",
    "\n",
    "$$\n",
    "V_{t + 1}(s) = \\max_{a}\\sum_{s'} T(s, a, s')[R(s, a, s') + \\gamma V_t(s')]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, gamma):\n",
    "    V = np.zeros(env.shape)\n",
    "    while True:\n",
    "        V_ = V\n",
    "        for s in env.S:\n",
    "            V_[s] = sum([p * (env.R(s, a) + gamma * V[env.T(s, a)]) \n",
    "                         for a, p in enumerate(policy[s])])\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Iteration\n",
    "Policy Iteration algorithm follows the steps below:\n",
    "1. Initialize the agent's policy ($\\forall s \\in S, \\pi(s) \\in A$)\n",
    "2. Evaluate $\\pi$ for each state $s$.\n",
    "3. Improve $\\pi$ for each state $s$; go to 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-+-+-+-+\n",
      "|^|^|^|^|\n",
      "+-+-+-+-+\n",
      "|^|^|^|^|\n",
      "+-+-+-+-+\n",
      "|^|^|^|^|\n",
      "+-+-+-+-+\n"
     ]
    }
   ],
   "source": [
    "def init_policy(env):\n",
    "    \"\"\" Initialize the policy with 4 possible actions for each state with the\n",
    "    same probability (1.0 / 4 actions = 0.25). \"\"\"\n",
    "    r, c = env.shape\n",
    "    return np.full((r, c, 4), 0.25)\n",
    "\n",
    "pi = init_policy(grids)\n",
    "grids.show(policy=pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.20431734  0.05373022  0.20431734 -0.03712508]\n",
      " [-0.284497   -0.06847337 -0.284497   -0.09822688]\n",
      " [-0.09756946 -0.03252315 -0.09756946 -0.36596606]]\n"
     ]
    }
   ],
   "source": [
    "def eval_policy(policy, env, gamma, theta=1e-8):\n",
    "    \"\"\" Evaluate the argument policy. \"\"\"\n",
    "    V = np.zeros(env.shape)\n",
    "    while True:\n",
    "        delta = 0.0\n",
    "        for s in env.S:\n",
    "            v = V[s]\n",
    "            V[s] = sum([p * (env.R(s, a) + gamma * V[env.T(s, a)])\n",
    "                        for a, p in enumerate(policy[s])])\n",
    "            delta = max(delta, abs(v - V[s]))\n",
    "        if delta < theta:\n",
    "            return V\n",
    "        \n",
    "print(eval_policy(pi, grids, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impr_policy(policy, env, gamma):\n",
    "    \"\"\" Improve the argument policy. \"\"\"\n",
    "    policy_stable = True\n",
    "    V = eval_policy(policy, env, gamma)\n",
    "    for s in env.S:\n",
    "        old_action = np.argmax(policy[s])\n",
    "        new_action = np.argmax([p * (env.R(s) + gamma * V[env.T(s, a)])\n",
    "                               for a, p in enumerate(policy[s])])\n",
    "        if new_action != old_action:\n",
    "            policy_stable = False\n",
    "        policy[s] = np.eye(4)[new_action]\n",
    "    return policy_stable, V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can put these functions together to complete Policy Iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-+-+-+-+\n",
      "|<|<|>|>|\n",
      "+-+-+-+-+\n",
      "|^|^|^|^|\n",
      "+-+-+-+-+\n",
      "|^|^|^|^|\n",
      "+-+-+-+-+\n",
      "[[ 8.99999991  8.09999992  8.99999991  9.99999991]\n",
      " [ 8.09999992  7.28999993  8.09999992  7.99999992]\n",
      " [ 7.28999993  0.          7.28999993  7.19999993]]\n"
     ]
    }
   ],
   "source": [
    "def policy_iteration(env, gamma):\n",
    "    pi = init_policy(env)\n",
    "    done = False\n",
    "    while not done:\n",
    "        done, V = impr_policy(pi, env, gamma)\n",
    "    return pi, V\n",
    "\n",
    "pi, V = policy_iteration(grids, 0.9)\n",
    "grids.show(policy=pi)\n",
    "print(V)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
