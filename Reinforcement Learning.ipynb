{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "Jin Yeom (jinyeom@utexas.edu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll study and experiment with basics of reinforcement learning (RL). We're going to first talk about RL techniques that are based on known MDPs, i.e., the agent has an access to the transition and reward function, so that it can evaluate each state directly; then we'll talk about methods with unknown MDPs, where the agent has to explore the environment to learn an optimal behavior.\n",
    "\n",
    "Note that this notebook is based on Sutton and Barto's book __Reinforcement Learning: An introduction__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iteration and Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look at traditional AI methods (what we now call Good Old-Fashion Artificial Intelligence, or GOFAI) based on search algorithms, they are often designed with an assumption that the model of the environment is completely exposed to the agent. In other words, the agent is given both the transition function and the reward function. So, the agent is often able to search through different states and plan an optimal path to maximize the utility.\n",
    "\n",
    "In this section, we're going to start from there. The model is given to the agent; but rather than searching and planning to build its policy, it will learn to evaluate each state, so that the agent can later decide whether it should be in a certain state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Bellman Equation\n",
    "\n",
    "One key idea for the two algorithms in this section is the Bellman Equation (shown below).\n",
    "\n",
    "$$\n",
    "v_*(s) = \\max_{a}\\mathbb{E}[R_{t + 1} + \\gamma v_*(S_{t + 1}) \\space | \\space S_t = s, A_t = a]\n",
    "$$\n",
    "\n",
    "An important thing to notice in this equation is that this equation is recursive, i.e., the value of a state is determine by that of its next state, and the value of the next state by the next, and so on. Intuitively, what this equation is telling is, for each state, the optimal value is given by an action that ends up with the highest average value (if you're familiar with search-based AI methods, it's exactly like the Expectimax method)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Iteration\n",
    "\n",
    "Another observation that the Bellman Eqaution suggests is that, in order to evaluate each state, the agent must try some sequence of further actions and find out what happens. But rather than searching for the best sequence of actions by trying many different scenarios, we would like the agent to learn the best sequence of actions on its own. In other words, the agent will try some sequence of actions, evaluate it, and fix itself to improve itself. This process is called **Policy Iteration**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try implementing Policy Iteration for a small problem called GridWorld. In this problem, the agent has to move from a grid to another to reach the goal, while avoiding fire. The code for GridWorld is included separately in this repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scripts import gridworld as gw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with a common example grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-+-+-+-+\n",
      "| | | |o|\n",
      "+-+-+-+-+\n",
      "| |#| |x|\n",
      "+-+-+-+-+\n",
      "| | | | |\n",
      "+-+-+-+-+\n"
     ]
    }
   ],
   "source": [
    "# Aliases for easier interpretation of each grid type.\n",
    "D, W, G, F = gw.GRID, gw.WALL, gw.GOAL, gw.FIRE\n",
    "\n",
    "grid = gw.GridWorld(np.array([[D, D, D, G],\n",
    "                              [D, W, D, F],\n",
    "                              [D, D, D, D]]))\n",
    "\n",
    "# Visualize the grids.\n",
    "# Empty grids are ones that are reachable, `#` shows a wall,\n",
    "# `o` is the good terminal state (goal), and `x` is the bad\n",
    "# terminal state (fire).\n",
    "grid.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
