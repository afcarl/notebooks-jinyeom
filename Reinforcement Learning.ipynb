{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "Jin Yeom (jinyeom@utexas.edu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll study and experiment with basics of reinforcement learning (RL). We're going to first talk about RL techniques that are based on known MDPs, i.e., every state, transition from one to another, and its reward is exposed to the agent; then we'll talk about methods with unknown MDPs, where the agent has to explore the environment to approximate the MDP.\n",
    "\n",
    "Note that this notebook is based on Sutton and Barto's book __Reinforcement Learning: An introduction__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iteration and Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look at traditional AI methods (what we now call Good Old-Fashion Artificial Intelligence, or GOFAI) based on search algorithms, they are often designed with an assumption that the model of the environment is completely exposed to the agent. In other words, the agent has a direct access to all states. So, the agent is often able to search through different states and plan an optimal path to maximize the utility.\n",
    "\n",
    "In this section, we're going to start from there. The model is given to the agent; but rather than searching and planning to build its policy, it will explore and evaluate each state, so that the agent can learn to decide whether it should be in a certain state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Bellman Equation\n",
    "\n",
    "One key idea for the two algorithms in this section is the Bellman Equation (shown below).\n",
    "\n",
    "$$\n",
    "v_*(s) = \\max_{a}\\mathbb{E}[R_{t + 1} + \\gamma v_*(S_{t + 1}) \\space | \\space S_t = s, A_t = a]\n",
    "$$\n",
    "\n",
    "An important thing to notice in this equation is that this equation is recursive, i.e., the value of a state is determine by that of its next state, and the value of the next state by the next, and so on. Intuitively, what this equation is telling is, for each state, the optimal value is given by an action that ends up with the highest expected value (if you're familiar with search-based AI methods, it's exactly like the Expectimax method)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Iteration\n",
    "\n",
    "Another observation that the Bellman Eqaution suggests is that, in order to evaluate each state, the agent must try some sequence of further actions and find out what happens. But rather than searching for the best sequence of actions by trying every possible scenario, we would like the agent to learn the best sequence of actions, by learning where to be and where not to be. In other words, the agent will try some sequence of actions, evaluate it, fix the sequence to improve the score, and repeat. This process is called **Policy Iteration**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try implementing Policy Iteration for a small problem called GridWorld. In this problem, the agent moves from a grid to another to reach the goal, while avoiding fire. The code for GridWorld is included separately in this repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from scripts import gridworld as gw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with a common example grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-+-+-+-+\n",
      "| | | |o|\n",
      "+-+-+-+-+\n",
      "| |#| |x|\n",
      "+-+-+-+-+\n",
      "| | | | |\n",
      "+-+-+-+-+\n"
     ]
    }
   ],
   "source": [
    "# Aliases for easier interpretation of each grid type.\n",
    "D, W, G, F = gw.GRID, gw.WALL, gw.GOAL, gw.FIRE\n",
    "\n",
    "grids = gw.GridWorld(np.array([[D, D, D, G],\n",
    "                               [D, W, D, F],\n",
    "                               [D, D, D, D]]))\n",
    "\n",
    "# Visualize the grids.\n",
    "# Empty grids are ones that are reachable, `#` shows a wall,\n",
    "# `o` is the good terminal state (goal), and `x` is the bad\n",
    "# terminal state (fire).\n",
    "grids.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned earlier, the steps in Policy Iteration algorithm are\n",
    "1. Initialize the agent's policy ($\\forall s \\in S, \\pi(s) \\in A$)\n",
    "2. Evaluate $\\pi$ for each state $s$.\n",
    "3. Improve $\\pi$ for each state $s$; go to 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-+-+-+-+\n",
      "|^|^|^|^|\n",
      "+-+-+-+-+\n",
      "|^|^|^|^|\n",
      "+-+-+-+-+\n",
      "|^|^|^|^|\n",
      "+-+-+-+-+\n"
     ]
    }
   ],
   "source": [
    "def init_policy(env):\n",
    "    \"\"\" Initialize the policy with 4 possible actions for each state with the\n",
    "    same probability (1.0 / 4 actions = 0.25). \"\"\"\n",
    "    r, c = env.shape\n",
    "    return np.full((r, c, 4), 0.25)\n",
    "\n",
    "pi = init_policy(grids)\n",
    "grids.show(policy=pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.10215867  0.02686511  0.10215867  0.98143746]\n",
      " [-0.1422485  -0.03423668 -0.1422485  -1.04911344]\n",
      " [-0.04878473 -0.01626157 -0.04878473 -0.18298303]]\n"
     ]
    }
   ],
   "source": [
    "def eval_policy(policy, env, gamma, theta=1e-8):\n",
    "    \"\"\" Evaluate the argument policy. \"\"\"\n",
    "    V = np.zeros(env.shape)\n",
    "    while True:\n",
    "        delta = 0.0\n",
    "        for s in env.S:\n",
    "            v = V[s]\n",
    "            V[s] = sum([p * (env.R(s) + gamma * V[env.T(s, a)]) \n",
    "                        for a, p in enumerate(policy[s])])\n",
    "            delta = max(delta, abs(v - V[s]))\n",
    "        if delta < theta:\n",
    "            return V\n",
    "        \n",
    "print(eval_policy(pi, grids, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impr_policy(policy, env, gamma):\n",
    "    \"\"\" Improve the argument policy. \"\"\"\n",
    "    policy_stable = True\n",
    "    V = eval_policy(policy, env, gamma)\n",
    "    for s in env.S:\n",
    "        old_action = np.argmax(policy[s])\n",
    "        new_action = np.argmax([p * (env.R(s) + gamma * V[env.T(s, a)])\n",
    "                               for a, p in enumerate(policy[s])])\n",
    "        if new_action != old_action:\n",
    "            policy_stable = False\n",
    "        policy[s] = np.eye(4)[new_action]\n",
    "    return policy_stable, V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can put these functions together to complete Policy Iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-+-+-+-+\n",
      "|<|<|>|>|\n",
      "+-+-+-+-+\n",
      "|^|^|^|^|\n",
      "+-+-+-+-+\n",
      "|^|^|^|^|\n",
      "+-+-+-+-+\n",
      "[[ 2.33333331  1.63333332  2.33333331  3.33333331]\n",
      " [ 1.63333332  1.14333332  1.63333332  1.33333332]\n",
      " [ 1.14333332  0.          1.14333332  0.93333332]]\n"
     ]
    }
   ],
   "source": [
    "def policy_iteration(env, gamma):\n",
    "    pi = init_policy(env)\n",
    "    done = False\n",
    "    while not done:\n",
    "        done, V = impr_policy(pi, env, gamma)\n",
    "    return pi, V\n",
    "\n",
    "pi, V = policy_iteration(grids, 0.7)\n",
    "grids.show(policy=pi)\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
