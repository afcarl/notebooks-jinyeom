{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "Author: Jin Yeom (jinyeom@utexas.edu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll study and experiment with basics of reinforcement learning (RL).\n",
    "Note that this notebook is loosely based on Sutton and Barto's book __Reinforcement Learning: An introduction__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Bellman Equation\n",
    "\n",
    "One key idea for the two algorithms in this section is the Bellman Equation (shown below).\n",
    "\n",
    "$$\n",
    "V^*(s) = \\max_{a}\\sum_{s'} T(s, a, s')[R(s, a, s') + \\gamma V^*(s')]\n",
    "$$\n",
    "\n",
    "One important observation from this equation is that the optimal value of each state is determined by that of the next state. Dynamic programming can be used to efficiently solve the recurrence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For expermenting with these algorithms, we're going to use a GridWorld package developed by DeepMind, called pycolab."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
